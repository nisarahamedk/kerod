{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1jVKNjhdLFUQ"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade git+https://github.com/EmGarr/kerod.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CET-72i5EmKn"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../kerod/src/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ews_7qF9OBi8"
   },
   "source": [
    "# Download COCO/2017\n",
    "\n",
    "Download and preprocess COCO/2017 to the following format (required by od networks):\n",
    "\n",
    "```python\n",
    "dataset = {\n",
    "        'images' : A tensor of float32 and shape [1, height, widht, 3],\n",
    "        'images_info': A tensor of float32 and shape [1, 2] ,\n",
    "        'bbox': A tensor of float32 and shape [1, num_boxes, 4],\n",
    "        'labels': A tensor of int32 and shape [1, num_boxes],\n",
    "        'num_boxes': A tensor of int32 and shape [1, 1],\n",
    "        'weights': A tensor of float32 and shape [1, num_boxes]\n",
    "    }\n",
    "```\n",
    "\n",
    "If you need to download the dataset in a specific directory you can use the argument `data_dir` of `tfds.load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1cC2k8osNGFw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from kerod.dataset.preprocessing import preprocess, expand_dims_for_single_batch\n",
    "from kerod.core.standard_fields import BoxField\n",
    "\n",
    "ds_train, ds_info = tfds.load(name=\"coco/2017\", split=\"train\", shuffle_files=True, with_info=True)\n",
    "ds_train = ds_train.map(functools.partial(preprocess, bgr=True), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# Filter example with no boxes after preprocessing\n",
    "ds_train =  ds_train.filter(lambda x, y: tf.shape(y[BoxField.BOXES])[0] > 1)\n",
    "ds_train = ds_train.map(expand_dims_for_single_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test = tfds.load(name=\"coco/2017\", split=\"validation\", shuffle_files=False)\n",
    "ds_test = ds_test.map(\n",
    "    functools.partial(preprocess, horizontal_flip=False, bgr=True),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# Filter example with no boxes after preprocessing\n",
    "ds_test =  ds_test.filter(lambda x, y: tf.shape(y[BoxField.BOXES])[0] > 1)\n",
    "ds_test = ds_test.map(expand_dims_for_single_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fip6g2i-B3pi"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='coco',\n",
       "    full_name='coco/2017/1.1.0',\n",
       "    description=\"\"\"\n",
       "    COCO is a large-scale object detection, segmentation, and\n",
       "    captioning dataset.\n",
       "    \n",
       "    Note:\n",
       "     * Some images from the train and validation sets don't have annotations.\n",
       "     * Coco 2014 and 2017 uses the same images, but different train/val/test splits\n",
       "     * The test split don't have any annotations (only images).\n",
       "     * Coco defines 91 classes but the data only uses 80 classes.\n",
       "     * Panotptic annotations defines defines 200 classes but only uses 133.\n",
       "    \"\"\",\n",
       "    config_description=\"\"\"\n",
       "    \n",
       "    This version contains images, bounding boxes and labels for the 2017 version.\n",
       "    \n",
       "    \"\"\",\n",
       "    homepage='http://cocodataset.org/#home',\n",
       "    data_path='/home/nikasmohan/tensorflow_datasets/coco/2017/1.1.0',\n",
       "    download_size=25.20 GiB,\n",
       "    dataset_size=24.98 GiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
       "        'image/filename': Text(shape=(), dtype=tf.string),\n",
       "        'image/id': tf.int64,\n",
       "        'objects': Sequence({\n",
       "            'area': tf.int64,\n",
       "            'bbox': BBoxFeature(shape=(4,), dtype=tf.float32),\n",
       "            'id': tf.int64,\n",
       "            'is_crowd': tf.bool,\n",
       "            'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=80),\n",
       "        }),\n",
       "    }),\n",
       "    supervised_keys=None,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=40670, num_shards=64>,\n",
       "        'train': <SplitInfo num_examples=118287, num_shards=256>,\n",
       "        'validation': <SplitInfo num_examples=5000, num_shards=8>,\n",
       "    },\n",
       "    citation=\"\"\"@article{DBLP:journals/corr/LinMBHPRDZ14,\n",
       "      author    = {Tsung{-}Yi Lin and\n",
       "                   Michael Maire and\n",
       "                   Serge J. Belongie and\n",
       "                   Lubomir D. Bourdev and\n",
       "                   Ross B. Girshick and\n",
       "                   James Hays and\n",
       "                   Pietro Perona and\n",
       "                   Deva Ramanan and\n",
       "                   Piotr Doll{'{a}}r and\n",
       "                   C. Lawrence Zitnick},\n",
       "      title     = {Microsoft {COCO:} Common Objects in Context},\n",
       "      journal   = {CoRR},\n",
       "      volume    = {abs/1405.0312},\n",
       "      year      = {2014},\n",
       "      url       = {http://arxiv.org/abs/1405.0312},\n",
       "      archivePrefix = {arXiv},\n",
       "      eprint    = {1405.0312},\n",
       "      timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},\n",
       "      biburl    = {https://dblp.org/rec/bib/journals/corr/LinMBHPRDZ14},\n",
       "      bibsource = {dblp computer science bibliography, https://dblp.org}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "ds_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'images': <tf.Tensor: shape=(1, 800, 1199, 3), dtype=float32, numpy=\n",
       "  array([[[[59.      , 58.      , 60.      ],\n",
       "           [59.      , 58.      , 60.      ],\n",
       "           [59.      , 58.      , 60.      ],\n",
       "           ...,\n",
       "           [52.834473, 54.834473, 54.834473],\n",
       "           [52.30066 , 54.30066 , 54.30066 ],\n",
       "           [52.      , 54.      , 54.      ]],\n",
       "  \n",
       "          [[58.39875 , 57.39875 , 59.39875 ],\n",
       "           [58.48914 , 57.48914 , 59.48914 ],\n",
       "           [58.649605, 57.649605, 59.649605],\n",
       "           ...,\n",
       "           [52.081882, 54.081882, 54.081882],\n",
       "           [52.029503, 54.029503, 54.029503],\n",
       "           [52.      , 54.      , 54.      ]],\n",
       "  \n",
       "          [[57.33125 , 56.33125 , 58.33125 ],\n",
       "           [57.58212 , 56.58212 , 58.58212 ],\n",
       "           [58.027493, 57.027493, 59.027493],\n",
       "           ...,\n",
       "           [50.74568 , 52.74568 , 52.74568 ],\n",
       "           [51.548073, 53.548073, 53.548073],\n",
       "           [52.      , 54.      , 54.      ]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[31.171906, 48.171906, 81.171906],\n",
       "           [30.118626, 42.608616, 75.90929 ],\n",
       "           [28.248724, 32.732044, 66.56649 ],\n",
       "           ...,\n",
       "           [19.426603, 25.247404, 42.91635 ],\n",
       "           [16.48895 , 28.1817  , 44.78302 ],\n",
       "           [14.834381, 29.834381, 45.83438 ]],\n",
       "  \n",
       "          [[28.503143, 45.503143, 78.50314 ],\n",
       "           [27.931309, 40.4213  , 73.72197 ],\n",
       "           [26.916122, 31.399443, 65.23389 ],\n",
       "           ...,\n",
       "           [17.556646, 23.377447, 41.04639 ],\n",
       "           [15.473766, 27.166515, 43.767834],\n",
       "           [14.300629, 29.300629, 45.30063 ]],\n",
       "  \n",
       "          [[27.      , 44.      , 77.      ],\n",
       "           [26.699333, 39.189323, 72.48999 ],\n",
       "           [26.165554, 30.648874, 64.48332 ],\n",
       "           ...,\n",
       "           [16.503418, 22.324219, 39.993164],\n",
       "           [14.901978, 26.594727, 43.196045],\n",
       "           [14.      , 29.      , 45.      ]]]], dtype=float32)>,\n",
       "  'images_information': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 800., 1199.]], dtype=float32)>},\n",
       " {'bbox': <tf.Tensor: shape=(1, 3, 4), dtype=float32, numpy=\n",
       "  array([[[ 375.96252,  320.77   ,  536.7494 ,  675.61774],\n",
       "          [ 530.3419 ,   59.27559,  791.007  ,  654.31683],\n",
       "          [ 521.34894,  665.08905,  800.     , 1179.2166 ]]], dtype=float32)>,\n",
       "  'label': <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[15, 72, 72]], dtype=int32)>,\n",
       "  'num_boxes': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[3]], dtype=int32)>,\n",
       "  'weights': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[1., 1., 1.]], dtype=float32)>})"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "next(iter(ds_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9tt34CM6P-gr"
   },
   "source": [
    "# Load and train the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r4o3t4PCLagO",
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://files.heuritech.com/raw_files/resnet50_tensorpack_conversion.h5\n",
      "95035392/95034120 [==============================] - 47s 0us/step\n",
      "Epoch 1/12\n",
      "<dtype: 'bool'> <dtype: 'float32'> <dtype: 'float32'>\n",
      "WARNING:tensorflow:From /home/nikasmohan/miniconda3/envs/bms/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "WARNING:tensorflow:From /home/nikasmohan/miniconda3/envs/bms/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "<dtype: 'bool'> <dtype: 'float32'> <dtype: 'float32'>\n",
      "/home/nikasmohan/miniconda3/envs/bms/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/faster_rcnn_fpn_resnet50pytorch/fast_rcnn/sub:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/faster_rcnn_fpn_resnet50pytorch/fast_rcnn/GatherV2_2:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"gradient_tape/faster_rcnn_fpn_resnet50pytorch/fast_rcnn/Shape_4:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "/home/nikasmohan/miniconda3/envs/bms/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/faster_rcnn_fpn_resnet50pytorch/fast_rcnn/sub_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/faster_rcnn_fpn_resnet50pytorch/fast_rcnn/GatherV2_5:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"gradient_tape/faster_rcnn_fpn_resnet50pytorch/fast_rcnn/Shape_5:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "/home/nikasmohan/miniconda3/envs/bms/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/faster_rcnn_fpn_resnet50pytorch/fast_rcnn/sub_2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/faster_rcnn_fpn_resnet50pytorch/fast_rcnn/GatherV2_8:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"gradient_tape/faster_rcnn_fpn_resnet50pytorch/fast_rcnn/Shape_6:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "/home/nikasmohan/miniconda3/envs/bms/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/faster_rcnn_fpn_resnet50pytorch/fast_rcnn/sub_3:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/faster_rcnn_fpn_resnet50pytorch/fast_rcnn/GatherV2_11:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"gradient_tape/faster_rcnn_fpn_resnet50pytorch/fast_rcnn/Shape_7:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "<dtype: 'bool'> <dtype: 'float32'> <dtype: 'float32'>\n",
      "<dtype: 'bool'> <dtype: 'float32'> <dtype: 'float32'>\n",
      "      1/Unknown - 37s 37s/step - loss: 72.8336 - rpn_recall: 0.3667 - region_proposal_network_classification_loss: 7.8147 - region_proposal_network_localization_loss: 2.7242 - accuracy: 0.0000e+00 - fg_accuracy: 0.0000e+00 - false_negative: 0.0000e+00 - fast_rcnn_classification_loss: 61.2057 - fast_rcnn_localization_loss: 0.1084"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6f6ed5a9daa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m ]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel_faster_rcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/bms/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bms/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bms/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bms/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bms/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bms/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/envs/bms/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kerod.core.standard_fields import BoxField\n",
    "from kerod.core.learning_rate_schedule import LearningRateScheduler\n",
    "from kerod.model import factory\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "# Number of classes of COCO\n",
    "classes = ds_info.features['objects']['label'].names\n",
    "num_classes = len(classes)\n",
    "\n",
    "model_faster_rcnn = factory.build_model(num_classes)\n",
    "base_lr = 0.02\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=base_lr, momentum=0.9)\n",
    "model_faster_rcnn.compile(optimizer=optimizer, loss=None)\n",
    "\n",
    "#The numbering of epochs (LearningRateScheduler) starts at 0.\n",
    "# Which means the decrease will happens on the epoch 9:\n",
    "#(8 + 1: numbering of fit logging starts at 1)\n",
    "callbacks = [\n",
    "    LearningRateScheduler(base_lr, 1, epochs=[8, 10]),\n",
    "    TensorBoard(),\n",
    "    ModelCheckpoint('checkpoints/')\n",
    "]\n",
    "\n",
    "model_faster_rcnn.fit(ds_train, validation_data=ds_test, epochs=12, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights for the serving\n",
    "model_faster_rcnn.save_weights('final_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export a saved model for serving purposes\n",
    "model_faster_rcnn.export_for_serving('serving')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from od.utils.drawing import BoxDrawer\n",
    "\n",
    "drawer = BoxDrawer(classes)\n",
    "\n",
    "for i, example in enumerate(ds_val):\n",
    "    inputs, ground_truths = example\n",
    "    out = model_faster_rcnn.predict_on_batch(inputs)\n",
    "    boxes, scores, labels, valid_detections = out\n",
    "    # Will draw the results\n",
    "    drawer(\n",
    "        inputs['images'],\n",
    "        boxes,\n",
    "        scores=scores,\n",
    "        labels=labels,\n",
    "        num_valid_detections=valid_detections\n",
    "    )\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CD14aaUMEudZ"
   },
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ec4-mdjcR_wy"
   },
   "outputs": [],
   "source": [
    "# Load TENSORBOARD\n",
    "%load_ext tensorboard\n",
    "# Start TENSORBOARD\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coco evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "ds_val, ds_info = tfds.load(name=\"coco/2017\", split=\"validation\", shuffle_files=False, with_info=True)\n",
    "# category_ids basicaly map the index 0 the id\n",
    "# e.g: 0 -> 1, 2 -> 3, 79 -> 90\n",
    "category_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Super dirty but the evaluation works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. perform the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from kerod.core.standard_fields import DatasetField, BoxField             \n",
    "from kerod.core.box_ops import convert_to_center_coordinates              \n",
    "from kerod.dataset.preprocessing import resize_to_min_dim                 \n",
    "                                                                       \n",
    "results = []                                                           \n",
    "                                                                       \n",
    "for i, example in enumerate(ds_val): \n",
    "    print(i)\n",
    "    # preprocess image\n",
    "    image = example['image'][:, :, ::-1]\n",
    "    image = resize_to_min_dim(image, 800.0, 1333.0)         \n",
    "    image_information = tf.cast(tf.shape(image)[:2], dtype=tf.float32) \n",
    "    inputs = {\n",
    "        'images': tf.expand_dims(image, axis=0),\n",
    "        'images_information':tf.expand_dims(image_information, axis=0)\n",
    "    }\n",
    "                                                                 \n",
    "    # predict                                                          \n",
    "    boxes, scores, labels, valid_detections = model_faster_rcnn.predict_on_batch(inputs)\n",
    "                                                                       \n",
    "    # Post processing and append to coco results                       \n",
    "    bbox = boxes[0] * tf.tile(\n",
    "        tf.expand_dims(tf.cast(example['image'].shape[:2], tf.float32), axis=0),\n",
    "        [1, 2])                   \n",
    "    scores = scores[0]                                           \n",
    "    labels = labels[0]                                           \n",
    "    for i in range(valid_detections[0]):\n",
    "        # Convert from [y_min, x_min, y_max, x_max] to coco format [x_min, y_min, w, h]\n",
    "        sbox = bbox[i].numpy()\n",
    "        sbox = [sbox[1], sbox[0], sbox[3] - sbox[1], sbox[2] - sbox[0]]\n",
    "        res = {                                                        \n",
    "                'image_id': int(example['image/id']),                       \n",
    "                'category_id': category_ids[int(labels[i])],           \n",
    "                'bbox': [round(float(c), 4) for c in sbox],\n",
    "                'score': round(float(scores[i]), 4),                     \n",
    "            }                                                          \n",
    "        results.append(res)                                            \n",
    "                                                   \n",
    "                                                                       \n",
    "with open('coco_results.json', 'w') as f:                              \n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. install the coco library to compute the performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "!unzip annotations_trainval2017.zip\n",
    "!pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. compute the performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "with open('coco_results_corrected.json', 'r') as f:                              \n",
    "    results = json.load(f)\n",
    "coco = COCO('./annotations/instances_val2017.json')\n",
    "ret = {}\n",
    "\n",
    "cocoDt = coco.loadRes(results)\n",
    "cocoEval = COCOeval(coco, cocoDt, 'bbox')\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "coco_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3710jvsc74a57bd09e5b45f77013f2e5edbdd3f84cc58f2e6775f1adfb96c9a7644cc79999760d60",
   "display_name": "Python 3.7.10 64-bit ('bms': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "9e5b45f77013f2e5edbdd3f84cc58f2e6775f1adfb96c9a7644cc79999760d60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}